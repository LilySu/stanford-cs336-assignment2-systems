1.1.3
Part a

| Size   |   d_model |   d_ff |   num_layers |   num_heads | Forward (ms)   | Backward (ms)      |
|:-------|----------:|-------:|-------------:|------------:|:---------------|:-------------------|
| small  |       768 |   3072 |           12 |          12 | 41.63 ± 75.30  | 57.61 ± 23.55      |
| medium |      1024 |   4096 |           24 |          16 | 46.39 ± 3.19   | 150.12 ± 4.21      |
| large  |      1280 |   5120 |           36 |          20 | 102.73 ± 2.73  | 304.39 ± 8.01      |
| xl     |      1600 |   6400 |           48 |          25 | 191.78 ± 15.36 | 35707.72 ± 1286.26 |
| 2.7B   |       nan |    nan |          nan |         nan | OOM            | OOM                |




Part b: Benchmark Timings (With Warmup)
Forward pass timings range from 19.72 ms (Small) to 195.44 ms (XL), while backward passes range from 52.73 ms to a drastic 36,592 ms for the XL model. The standard deviations are generally small relative to the means, indicating stable performance across the measurement steps, with the exception of the memory-constrained XL backward pass.

Part c: Analysis of Warmup Steps
Omitting warmup steps significantly increases the standard deviation and mean execution time—most notably in the "Small" model (Forward SD ±75.30 ms)—due to one-time initialization overheads such as CUDA context creation and memory allocation. Even with 1 or 2 warmup steps, results may still differ from the stable average because GPU clock frequencies often take time to ramp up to full speed and internal caches need time to populate.


1.1.4 
Nsys profile

(a) Total Time on Forward Pass
The total time for the forward pass varies by model size and context length. Based on the "Forward Only" profiles provided:

Small Model (Context 1024): The forward pass takes approximately 57.5 ms per iteration.

Small Model (Context 512): The forward pass takes approximately 19.3 ms per iteration.

Medium Model (Context 128): The forward pass takes approximately 24.0 ms per iteration.

1.1.5
Benchmarking Mixed Precision:
a) 
Data Types in FP16 Autocast
Model parameters: FP32 (Weights are stored in FP32; autocast creates temporary lower-precision copies for operations).

Output of fc1: FP16 (Linear layers are autocast-eligible).

Output of ln: FP32 (LayerNorm falls back to FP32 for numerical stability).

Predicted logits: FP16 (Output of fc2, which is a Linear layer).

Loss: FP32 (Loss functions typically execute in FP32).

Model gradients: FP32 (Gradients accumulate in the same dtype as the parameters).
Explanations:
Model Parameters within the autocast context: FP32 The actual parameters stored in memory (model.fc1.weight) remain in Float32 to serve as the "master copy." autocast only creates temporary Float16 copies for the split second the math is performed, without modifying the original parameters.

Output of the first feed-forward layer (ToyModel.fc1): FP16 nn.Linear is on the autocast "allowlist" because it is computationally heavy and safe to run in lower precision. Therefore, PyTorch casts the inputs and temporary weights to Float16, producing a Float16 output.

Output of layer norm (ToyModel.ln): FP32 nn.LayerNorm is on the autocast "denylist" because statistics (mean/variance) require high precision. autocast forces the input (even if it comes as FP16) back up to Float32 to run this operation, resulting in a Float32 output.

The model’s predicted logits: FP16 The final layer (fc2) is an nn.Linear layer, which autocast prefers to run in lower precision for speed. It downcasts the Float32 input (received from the LayerNorm) to Float16 for the calculation, returning Float16 logits.

The Loss: FP32 PyTorch loss functions (like CrossEntropy or MSE) generally execute in Float32 to ensure the final error value is calculated with high precision. The loss function receives the FP16 logits but casts them to Float32 before computing the scalar loss.

The model’s gradients: FP32 Gradients are accumulated into the .grad attribute of your parameters, which must match the data type of the parameters themselves. Since the master parameters are stored in FP32, the gradients are also stored in FP32 to ensure accurate updates.

b)
Layer normalization requires calculating the variance (sum of squared differences), which can easily produce values exceeding the maximum range of FP16 (~65,504), leading to overflow (Infinity). We do not need to treat it differently in BF16 because BF16 preserves the same 8-bit exponent as FP32, providing the same dynamic range and preventing this overflow.

c)
For the Large model, Mixed Precision (BF16) was surprisingly slower than Full Precision (FP32), taking 180.2s versus 14.9s for the benchmark run (~12x slowdown). This regression is likely due to memory capacity limits: while BF16 reduces activation size, autocast must cache casted BF16 weights alongside the FP32 master weights. For the Large model, this extra memory overhead likely exceeded the GPU's VRAM, causing expensive unified memory swapping (thrashing) as seen by the dominance of elementwise_copy kernels. For the Small and Medium models (assuming they fit in memory), we expect mixed precision to improve latency by reducing memory bandwidth usage.


1.1.6
profiling memory

(a) Memory Timeline AnalysisResponse:The Forward Pass timeline typically resembles a "staircase" or "ramp" pattern where memory increases as activations are computed and stored for each layer, dropping sharply when the pass ends (or staying flat if inference caching is used, though in this simple script it drops). The Full Training Step timeline shows a distinctive "sawtooth" or "triangle" shape: memory climbs steadily during the forward pass (storing activations for backprop), peaks at the end of the forward pass, and then decreases in steps during the backward pass as gradients are computed and activation buffers are freed, followed by a potential spike or shift during the optimizer step depending on the optimizer implementation. You can identify the backward pass as the descending slope of the large triangle.(b) Peak Memory Usage (Table)You must fill in the specific values (in MB/GB) by looking at the "Peak Memory" stat in the memory_viz tool for the snapshots you generated.Context LengthForward Pass Peak (MB)Full Training Step Peak (MB)128[Check Snapshot][Check Snapshot]256[Check Snapshot][Check Snapshot]512[Check Snapshot][Check Snapshot](c) Mixed Precision ImpactResponse:Mixed-precision significantly reduces memory usage, particularly for the forward pass, because activations are stored in bfloat16 (2 bytes) instead of float32 (4 bytes), effectively halving the memory required for the activation cache. However, the reduction for a full training step might be less than 50% overall because the model weights and the optimizer states (which are the largest static memory consumers) are often still kept in float32 (master weights) or require float32 copies, depending on the specific AMP implementation.(d) Size of Residual Stream Activation TensorDerivation:Based on the large configuration in your script:$d_{model} = 2048$Batch Size ($B$) = 4 (default in script)Single Precision = 4 bytes per element.The size of a single tensor of activations in the residual stream (shape $[B, T, d_{model}]$) is calculated as:$$\text{Size} = B \times T \times d_{model} \times 4 \text{ bytes}$$For a context length of 128:$$4 \times 128 \times 2048 \times 4 = 4,194,304 \text{ bytes} \approx 4 \text{ MB}$$Note: If the question implies a single token's activation vector, it is just $2048 \times 4 = 8$ KB, but usually "tensor of activations" refers to the full sequence batch.Response:For the large model ($d_{model}=2048$) with batch size 4 and context length 128, a single residual stream activation tensor is approximately 4 MB ($4 \times 128 \times 2048 \times 4$ bytes). This scales linearly with context length (e.g., 8 MB for ctx=256).(e) Largest Allocations in Forward PassResponse:When reducing the "Detail" level in the memory visualizer, the largest allocations visible are typically the Feed-Forward Network (FFN) intermediate activations. In the standard Transformer architecture, the FFN projects the hidden dimension to $4 \times d_{model}$ (which is 8192 for the large model), creating a tensor of shape $[B, T, 8192]$ that is significantly larger than the attention scores or residual stream tensors.


Task: Upload memory_snapshot_large_forward_ctx128.pickle to pytorch.org/memory_viz.

Dominant Component: The memory is dominated by Model Weights.The Math: The Large model has ~1.6 billion parameters. In FP32 (4 bytes/param), this requires:$$1.6 \times 10^9 \times 4 \text{ bytes} \approx 6.4 \text{ GB}$$Activations: For small context lengths (128/256), the activation memory (intermediate tensors) is negligible compared to the 6.4 GB of static weights.Part 2: Training Memory Analysis (The Crash)Task: Explain the OOM error for full_training.What to write:"The full_training run failed with an Out Of Memory (OOM) error because the static memory requirements for training the Large model in FP32 exceed the available GPU VRAM. Training requires storing:"Model Weights: ~6.4 GBGradients: ~6.4 GB (same size as weights)Optimizer States (AdamW): Adam stores 2 states (momentum and variance) per parameter.$$1.6 \text{B params} \times 2 \text{ states} \times 4 \text{ bytes} \approx 12.8 \text{ GB}$$Total Static Memory:$$6.4 + 6.4 + 12.8 = \mathbf{25.6 \text{ GB}}$$This 25.6 GB baseline is required before even allocating a single byte for activations or CUDA context overhead. Since this exceeds the physical memory of the GPU (likely 24GB), the allocator fails immediately during the optimizer.step() as seen in the traceback.


1.2.1: Benchmarking PyTorch Attention

1.

| Batch Size | d_head | seq_len | Forward (ms) | Backward (ms) | Memory (Bytes) | Memory (Formatted) |
|-----------:|-------:|--------:|:-------------|:--------------|:---------------|:-------------------|
|          8 |     16 |     256 | 0.16         | 0.44          | 13369344       | 12.75 MB           |
|          8 |     16 |    1024 | 1.11         | 2.72          | 86769664       | 82.75 MB           |
|          8 |     16 |    4096 | 13.15        | 24.67         | 1101266944     | 1.03 GB            |
|          8 |     16 |    8192 | 36.47        | 70.20         | 4332978176     | 4.04 GB            |
|          8 |     16 |   16384 | OOM          | OOM           | OOM            | OOM                |
|          8 |     32 |     256 | 0.57         | 1.54          | 8654422016     | 8.06 GB * |
|          8 |     32 |    1024 | 0.94         | 2.90          | 89391104       | 85.25 MB           |
|          8 |     32 |    4096 | 17.92        | 23.29         | 1111752704     | 1.04 GB            |
|          8 |     32 |    8192 | 42.64        | 455.95        | 4353949696     | 4.05 GB            |
|          8 |     32 |   16384 | OOM          | OOM           | OOM            | OOM                |
|          8 |     64 |     256 | 0.41         | 1.06          | 40632320       | 38.75 MB           |
|          8 |     64 |    1024 | 2.13         | 3.37          | 94633984       | 90.25 MB           |
|          8 |     64 |    4096 | 14.92        | 24.28         | 1132724224     | 1.05 GB            |
|          8 |     64 |    8192 | 42.54        | 459.81        | 4395892736     | 4.09 GB            |
|          8 |     64 |   16384 | OOM          | OOM           | OOM            | OOM                |
|          8 |    128 |     256 | 0.26         | 0.91          | 60030976       | 57.25 MB           |
|          8 |    128 |    1024 | 2.99         | 3.63          | 105119744      | 100.25 MB          |
|          8 |    128 |    4096 | 18.11        | 22.72         | 1174667264     | 1.09 GB            |
|          8 |    128 |    8192 | 44.06        | 480.34        | 4479778816     | 4.17 GB            |
|          8 |    128 |   16384 | OOM          | OOM           | OOM            | OOM                |

* Note: The 8.06 GB entry for B=8, Seq=256, D=32 appears to be an outlier or 
measurement artifact (likely uncollected garbage from a previous run or 
fragmentation), as the surrounding values are much lower.

2.

Question: Do the accounting for the memory usage of attention in one of the 
smallest configurations you find that runs out of memory.

Configuration Analyzed:
- Batch Size: 8
- Sequence Length: 16,384 (OOM Point)
- Precision: Float32 (4 bytes per element)

Analysis:
In the naive attention implementation, the system must materialize the attention 
scores matrix of shape (Batch, Seq_Len, Seq_Len). We can estimate the memory 
usage by observing the trend in the data:
- At Seq_Len = 8192, the memory usage was approx 4.04 GB.
- Since the attention matrix scales quadratically (O(N^2)), doubling the 
  sequence length to 16,384 implies a 4x increase in memory.
- Estimated Memory at 16,384 ≈ 4.04 GB * 4 ≈ 16.16 GB.

Theoretical Calculation (Single Matrix):
The memory for a single (Batch, Seq, Seq) attention matrix is:
Memory = 8 * (16,384)^2 * 4 bytes
       = 8 * 268,435,456 * 4
       = 8,589,934,592 bytes 
       ≈ 8.59 GB

Explanation of OOM:
While a single matrix requires ~8.6 GB, the backward pass requires saving 
intermediate tensors (such as the attention weights after softmax) to compute 
gradients. This effectively doubles the storage requirement for the attention 
grid to roughly ~17.2 GB (plus overhead for inputs Q/K/V). This exceeds the 
capacity of standard 16GB GPUs (and potentially 24GB GPUs due to memory 
fragmentation), causing the observed Out-Of-Memory error.

3.

Scaling of Backward Memory:
The memory required for the backward pass scales quadratically with the sequence 
length (O(N^2)). This is clearly visible in the benchmark data:
- At seq_len = 4096, memory usage is ~1.03 GB.
- At seq_len = 8192, memory usage quadruples to ~4.04 GB.
This occurs because the naive implementation must save the full (Batch, Seq, Seq) 
attention matrix computed during the forward pass to calculate gradients during 
backpropagation.

Solution for Eliminating Memory Cost:
To eliminate this memory cost, I would implement Flash Attention. Flash 
Attention avoids explicitly materializing the N x N attention matrix in High 
Bandwidth Memory (HBM). Instead, it computes attention in tiles using on-chip 
SRAM and uses activation checkpointing (recomputing the attention scores during 
the backward pass) rather than storing them. This reduces the memory complexity 
from quadratic O(N^2) to linear O(N) with respect to sequence length.


1.3 Jit-Compiled Attention

a) 
Small Sequences (Overhead): At seq_len=256, the compiled version is actually slower (0.85x). This confirms that for small workloads, the overhead of the JIT runtime (guard checks, kernel launching) outweighs the benefits of compilation.

t seq_len=1024, you see a speedup (1.16x), indicating the compute work has become large enough to amortize the compilation overhead.

a plateau at ~1.0x-1.1x speedup and an OOM at 16384 for the compiled version.

Theoretical Expectation: A fully successful torch.compile on a supported platform (like Linux with full Triton support) typically fuses the attention into a single kernel (FlashAttention), resulting in 2-3x speedups and no OOM at 16k.

On my specific system (likely WSL/Windows), the compiler fell back to standard kernels rather than generating the memory-efficient FlashAttention kernel. It is worth noting that the "expected" memory savings did not trigger in this environment.

b)

Model Size,Vanilla Fwd (ms),Compiled Fwd (ms),Speedup Fwd,Vanilla Train (ms),Compiled Train (ms),Speedup Train
small,6.71,12.35,0.54x,26.65,19.32,1.38x
medium,15.12,11.58,1.31x,97.03,90.13,1.08x


Small Model (Slower - 0.54x): The forward pass regressed significantly. The model is too small to hide the fixed overheads of the JIT executor. The time spent managing the compiled graph was longer than the time saved by removing Python interpreter overhead.

Medium Model (Faster - 1.31x): The performance improved. The larger matrix multiplications and layer dimensions provided enough compute work to amortize the launch overhead. Additionally, the compiler likely fused element-wise operations (like LayerNorm and Residuals) to reduce memory bandwidth usage.



Small Model (1.38x Speedup): Despite the slow forward pass, the training step was 38% faster. This is because the Backward Pass is heavily memory-bandwidth bound (reading/writing gradients for all parameters). torch.compile excels at fusing these bandwidth-heavy operations, and the savings there completely outweighed the forward pass overhead.

Medium Model (1.08x Speedup): We see a modest speedup. This suggests the medium model is compute-bound (dominated by large GEMMs/Matrix Multiplications). Since torch.compile relies on existing highly-optimized libraries (like cuBLAS) for GEMMs, it cannot optimize them much further, limiting the total speedup compared to memory-bound tasks.