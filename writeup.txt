1.1.3
Part a

| Size   |   d_model |   d_ff |   num_layers |   num_heads | Forward (ms)   | Backward (ms)      |
|:-------|----------:|-------:|-------------:|------------:|:---------------|:-------------------|
| small  |       768 |   3072 |           12 |          12 | 41.63 ± 75.30  | 57.61 ± 23.55      |
| medium |      1024 |   4096 |           24 |          16 | 46.39 ± 3.19   | 150.12 ± 4.21      |
| large  |      1280 |   5120 |           36 |          20 | 102.73 ± 2.73  | 304.39 ± 8.01      |
| xl     |      1600 |   6400 |           48 |          25 | 191.78 ± 15.36 | 35707.72 ± 1286.26 |
| 2.7B   |       nan |    nan |          nan |         nan | OOM            | OOM                |




Part b: Benchmark Timings (With Warmup)
Forward pass timings range from 19.72 ms (Small) to 195.44 ms (XL), while backward passes range from 52.73 ms to a drastic 36,592 ms for the XL model. The standard deviations are generally small relative to the means, indicating stable performance across the measurement steps, with the exception of the memory-constrained XL backward pass.

Part c: Analysis of Warmup Steps
Omitting warmup steps significantly increases the standard deviation and mean execution time—most notably in the "Small" model (Forward SD ±75.30 ms)—due to one-time initialization overheads such as CUDA context creation and memory allocation. Even with 1 or 2 warmup steps, results may still differ from the stable average because GPU clock frequencies often take time to ramp up to full speed and internal caches need time to populate.


1.1.4 
Nsys profile

(a) Total Time on Forward Pass
The total time for the forward pass varies by model size and context length. Based on the "Forward Only" profiles provided:

Small Model (Context 1024): The forward pass takes approximately 57.5 ms per iteration.

Small Model (Context 512): The forward pass takes approximately 19.3 ms per iteration.

Medium Model (Context 128): The forward pass takes approximately 24.0 ms per iteration.

1.1.5
Benchmarking Mixed Precision:
Model Parameters within the autocast context: FP32 The actual parameters stored in memory (model.fc1.weight) remain in Float32 to serve as the "master copy." autocast only creates temporary Float16 copies for the split second the math is performed, without modifying the original parameters.

Output of the first feed-forward layer (ToyModel.fc1): FP16 nn.Linear is on the autocast "allowlist" because it is computationally heavy and safe to run in lower precision. Therefore, PyTorch casts the inputs and temporary weights to Float16, producing a Float16 output.

Output of layer norm (ToyModel.ln): FP32 nn.LayerNorm is on the autocast "denylist" because statistics (mean/variance) require high precision. autocast forces the input (even if it comes as FP16) back up to Float32 to run this operation, resulting in a Float32 output.

The model’s predicted logits: FP16 The final layer (fc2) is an nn.Linear layer, which autocast prefers to run in lower precision for speed. It downcasts the Float32 input (received from the LayerNorm) to Float16 for the calculation, returning Float16 logits.

The Loss: FP32 PyTorch loss functions (like CrossEntropy or MSE) generally execute in Float32 to ensure the final error value is calculated with high precision. The loss function receives the FP16 logits but casts them to Float32 before computing the scalar loss.

The model’s gradients: FP32 Gradients are accumulated into the .grad attribute of your parameters, which must match the data type of the parameters themselves. Since the master parameters are stored in FP32, the gradients are also stored in FP32 to ensure accurate updates.